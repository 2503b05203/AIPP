{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe160b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (15, 3)\n",
      "\n",
      "Original columns: ['review_id', 'review_text', 'rating']\n",
      "\n",
      "First few rows of original data:\n",
      "   review_id                review_text  rating\n",
      "0          1      <p>Amazing movie!</p>     8.0\n",
      "1          2  Terrible acting & plot!!!     2.0\n",
      "2          3      <p>Amazing movie!</p>     NaN\n",
      "3          4  Terrible acting & plot!!!     8.0\n",
      "4          5      <p>Amazing movie!</p>     5.0\n",
      "\n",
      "Data types:\n",
      "review_id        int64\n",
      "review_text     object\n",
      "rating         float64\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "review_id      0\n",
      "review_text    0\n",
      "rating         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\subha\\Documents\\m.tech\\AIPP\\assignment-17\\movie_reviews-1.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Original dataset shape:\", df.shape)\n",
    "print(\"\\nOriginal columns:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows of original data:\")\n",
    "print(df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3aa25b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning review text...\n",
      "\n",
      "Sample of cleaned text:\n",
      "                 review_text   review_text_cleaned\n",
      "0      <p>Amazing movie!</p>         amazing movie\n",
      "1  Terrible acting & plot!!!  terrible acting plot\n",
      "2      <p>Amazing movie!</p>         amazing movie\n",
      "3  Terrible acting & plot!!!  terrible acting plot\n",
      "4      <p>Amazing movie!</p>         amazing movie\n",
      "5  Terrible acting & plot!!!  terrible acting plot\n",
      "6      <p>Amazing movie!</p>         amazing movie\n",
      "7  Terrible acting & plot!!!  terrible acting plot\n",
      "8      <p>Amazing movie!</p>         amazing movie\n",
      "9  Terrible acting & plot!!!  terrible acting plot\n"
     ]
    }
   ],
   "source": [
    "# Get stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing HTML tags, punctuation, special symbols, and stopwords.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters and punctuation, keep only alphanumeric and spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and single character tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
    "    \n",
    "    # Join tokens back\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Clean the review_text column\n",
    "print(\"Cleaning review text...\")\n",
    "df['review_text_cleaned'] = df['review_text'].apply(clean_text)\n",
    "\n",
    "print(\"\\nSample of cleaned text:\")\n",
    "print(df[['review_text', 'review_text_cleaned']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b946931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values in numerical columns...\n",
      "Numerical columns found: ['rating']\n",
      "\n",
      "Missing values after handling:\n",
      "rating    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in numerical columns\n",
    "print(\"Handling missing values in numerical columns...\")\n",
    "\n",
    "# Identify numerical columns (excluding review_id if it's just an identifier)\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'review_id' in numerical_cols:\n",
    "    numerical_cols.remove('review_id')\n",
    "\n",
    "print(\"Numerical columns found:\", numerical_cols)\n",
    "\n",
    "# Fill missing values in numerical columns with median (more robust than mean)\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        median_value = df[col].median()\n",
    "        df[col].fillna(median_value, inplace=True)\n",
    "        print(f\"Filled {df[col].isnull().sum()} missing values in '{col}' with median: {median_value}\")\n",
    "\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(df[numerical_cols].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fa56720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for date/timestamp columns...\n",
      "No date/timestamp columns found in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Convert date/timestamp columns to datetime format and extract features\n",
    "print(\"Checking for date/timestamp columns...\")\n",
    "\n",
    "# Check all columns for potential date/timestamp format\n",
    "date_columns = []\n",
    "for col in df.columns:\n",
    "    # Skip if it's review_id or text columns\n",
    "    if col in ['review_id', 'review_text', 'review_text_cleaned']:\n",
    "        continue\n",
    "    \n",
    "    # Try to detect if column contains date-like strings\n",
    "    sample_values = df[col].dropna().head(10)\n",
    "    if len(sample_values) > 0:\n",
    "        # Check if values look like dates\n",
    "        date_patterns = [\n",
    "            r'\\d{4}-\\d{2}-\\d{2}',  # YYYY-MM-DD\n",
    "            r'\\d{2}/\\d{2}/\\d{4}',  # MM/DD/YYYY\n",
    "            r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}',  # YYYY-MM-DD HH:MM:SS\n",
    "        ]\n",
    "        for pattern in date_patterns:\n",
    "            if sample_values.astype(str).str.contains(pattern, regex=True).any():\n",
    "                date_columns.append(col)\n",
    "                break\n",
    "\n",
    "# Also check column names that might indicate dates\n",
    "potential_date_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                       for keyword in ['date', 'time', 'timestamp', 'created', 'updated'])]\n",
    "\n",
    "date_columns = list(set(date_columns + potential_date_cols))\n",
    "\n",
    "if date_columns:\n",
    "    print(f\"Found potential date columns: {date_columns}\")\n",
    "    for col in date_columns:\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            # Extract hour if it's a datetime\n",
    "            df[f'{col}_hour'] = df[col].dt.hour\n",
    "            # Extract weekday (0=Monday, 6=Sunday)\n",
    "            df[f'{col}_weekday'] = df[col].dt.dayofweek\n",
    "            # Extract day name\n",
    "            df[f'{col}_weekday_name'] = df[col].dt.day_name()\n",
    "            print(f\"Converted '{col}' to datetime and extracted hour and weekday features\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not convert '{col}' to datetime: {e}\")\n",
    "else:\n",
    "    print(\"No date/timestamp columns found in the dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c740f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting duplicate reviews...\n",
      "Found 13 exact duplicate reviews based on review_text\n",
      "Found 13 duplicate reviews based on cleaned text\n",
      "\n",
      "Original shape: (15, 4)\n",
      "After removing duplicates: (2, 4)\n",
      "Removed 13 duplicate reviews\n",
      "\n",
      "Detected 1 potential spam reviews\n",
      "After removing spam: (1, 4)\n",
      "Total removed (duplicates + spam): 14 reviews\n"
     ]
    }
   ],
   "source": [
    "# Detect and remove duplicate reviews\n",
    "print(\"Detecting duplicate reviews...\")\n",
    "\n",
    "# Check for exact duplicates based on review_text\n",
    "duplicate_text = df.duplicated(subset=['review_text'], keep='first')\n",
    "print(f\"Found {duplicate_text.sum()} exact duplicate reviews based on review_text\")\n",
    "\n",
    "# Check for duplicates based on cleaned text\n",
    "duplicate_cleaned = df.duplicated(subset=['review_text_cleaned'], keep='first')\n",
    "print(f\"Found {duplicate_cleaned.sum()} duplicate reviews based on cleaned text\")\n",
    "\n",
    "# Remove duplicates based on cleaned text (more effective for detecting spam)\n",
    "df_cleaned = df[~duplicate_cleaned].copy()\n",
    "\n",
    "print(f\"\\nOriginal shape: {df.shape}\")\n",
    "print(f\"After removing duplicates: {df_cleaned.shape}\")\n",
    "print(f\"Removed {df.shape[0] - df_cleaned.shape[0]} duplicate reviews\")\n",
    "\n",
    "# Detect potential spam reviews\n",
    "# Spam indicators: very short reviews, repeated words, all caps, etc.\n",
    "def detect_spam(text):\n",
    "    \"\"\"\n",
    "    Detect potential spam reviews based on various indicators.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return True\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Very short reviews (less than 3 words)\n",
    "    if len(text.split()) < 3:\n",
    "        return True\n",
    "    \n",
    "    # Check for excessive repetition (same word repeated many times)\n",
    "    words = text.split()\n",
    "    if len(words) > 0:\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        max_repetition = max(word_counts.values())\n",
    "        if max_repetition > len(words) * 0.5:  # More than 50% repetition\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Mark spam reviews\n",
    "df_cleaned['is_spam'] = df_cleaned['review_text_cleaned'].apply(detect_spam)\n",
    "spam_count = df_cleaned['is_spam'].sum()\n",
    "print(f\"\\nDetected {spam_count} potential spam reviews\")\n",
    "\n",
    "# Remove spam reviews\n",
    "df_final = df_cleaned[~df_cleaned['is_spam']].copy()\n",
    "df_final = df_final.drop('is_spam', axis=1)\n",
    "\n",
    "print(f\"After removing spam: {df_final.shape}\")\n",
    "print(f\"Total removed (duplicates + spam): {df.shape[0] - df_final.shape[0]} reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7de12b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROCESSED DATASET\n",
      "================================================================================\n",
      "\n",
      "Final dataset shape: (1, 4)\n",
      "\n",
      "Columns: ['review_id', 'review_text', 'rating', 'review_text_cleaned']\n",
      "\n",
      "================================================================================\n",
      "First few rows of processed dataset:\n",
      "================================================================================\n",
      "   review_id                review_text  rating   review_text_cleaned\n",
      "1          2  Terrible acting & plot!!!     2.0  terrible acting plot\n",
      "\n",
      "================================================================================\n",
      "Dataset Summary:\n",
      "================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1 entries, 1 to 1\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   review_id            1 non-null      int64  \n",
      " 1   review_text          1 non-null      object \n",
      " 2   rating               1 non-null      float64\n",
      " 3   review_text_cleaned  1 non-null      object \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 40.0+ bytes\n",
      "None\n",
      "\n",
      "================================================================================\n",
      "Missing values in final dataset:\n",
      "================================================================================\n",
      "review_id              0\n",
      "review_text            0\n",
      "rating                 0\n",
      "review_text_cleaned    0\n",
      "dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Dataset cleaning completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display the processed dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"PROCESSED DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFinal dataset shape: {df_final.shape}\")\n",
    "print(f\"\\nColumns: {df_final.columns.tolist()}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"First few rows of processed dataset:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_final.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Dataset Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_final.info())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Missing values in final dataset:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_final.isnull().sum())\n",
    "\n",
    "# Return the processed dataset\n",
    "processed_dataset = df_final.copy()\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Dataset cleaning completed successfully!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d26966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
